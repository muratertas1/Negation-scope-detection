{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.32.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a token classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run on any token classification task, with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zVvslsfMIrIh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the task and model for natural language processing:\n",
    "# - 'task' specifies the type of task. It can be 'ner' (Named Entity Recognition), \n",
    "#   'pos' (Part-of-Speech tagging), or 'chunk' (chunking). Here, 'ner' is chosen.\n",
    "# - 'model_checkpoint' is set to use 'distilbert-base-uncased', which is a lightweight \n",
    "#   version of the BERT model and is not case-sensitive.\n",
    "# - 'batch_size' is set to 16, defining the number of samples to work through\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will load our data locally, define the feature columns, and represent it in DataseDict format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IreSlFmlIrIm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence, Value, Features, load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'lemmas', 'pos', 'constituency', 'negation_cue', 'negation_scope', 'negated_event'],\n",
      "        num_rows: 3780\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'lemmas', 'pos', 'constituency', 'negation_cue', 'negation_scope', 'negated_event'],\n",
      "        num_rows: 816\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'lemmas', 'pos', 'constituency', 'negation_cue', 'negation_scope', 'negated_event'],\n",
      "        num_rows: 1118\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries and modules.\n",
    "# pandas is used for data manipulation and analysis.\n",
    "# Dataset, DatasetDict, ClassLabel, Sequence, Value, Features are from the 'datasets' library for handling structured data.\n",
    "# We used ChatGPT, prompted the dataset output of the original template and step by step built the same structure for our data.\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the TSV file into a Pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Path to the .tsv file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reading data from a TSV file into a DataFrame with specified column names.\n",
    "    df = pd.read_csv(file_path, delimiter='\\t', header=None, names=[\n",
    "        'chapter', 'sentence_num', 'token_num', 'token', 'lemma', \n",
    "        'pos', 'constituency', 'negation_cue', 'negation_scope', 'negated_event'\n",
    "    ])\n",
    "\n",
    "    # Dropping rows where any column has missing values (NA).\n",
    "    df = df.dropna(how='any')\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_data(df, pos_label_mapping, negation_scope_label_mapping):\n",
    "    \"\"\"\n",
    "    Transform the DataFrame into a format suitable for machine learning models.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input data frame.\n",
    "    pos_label_mapping (dict): Mapping for POS tags to indices.\n",
    "    negation_scope_label_mapping (dict): Mapping for negation scopes to indices.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each representing a transformed data record.\n",
    "    \"\"\"\n",
    "    # Grouping the data by 'chapter' and 'sentence_num' for processing.\n",
    "    grouped = df.groupby(['chapter', 'sentence_num'])\n",
    "    transformed_data = []\n",
    "\n",
    "    # Transforming the grouped data into a format suitable for training.\n",
    "    for (chapter, sentence_num), group in grouped:\n",
    "        record = {\n",
    "            'id': f\"{chapter}_{sentence_num}\",\n",
    "            'tokens': group['token'].tolist(),\n",
    "            'lemmas': group['lemma'].tolist(),\n",
    "            'pos': [pos_label_mapping[label] for label in group['pos'].tolist()],\n",
    "            'constituency': group['constituency'].tolist(),\n",
    "            'negation_cue': group['negation_cue'].tolist(),\n",
    "            'negation_scope': [negation_scope_label_mapping[label] for label in group['negation_scope'].tolist()],\n",
    "            'negated_event': group['negated_event'].tolist()\n",
    "        }\n",
    "        transformed_data.append(record)\n",
    "    return transformed_data\n",
    "\n",
    "# Paths to the training, testing, and development data files.\n",
    "training_file_path = './data/bert_input_training.tsv' # Update with your file path\n",
    "test_file_path = './data/bert_input_test.tsv' # Update with your file path\n",
    "dev_file_path = './data/bert_input_dev.tsv'\n",
    "\n",
    "# Loading the data from the specified file paths.\n",
    "training_data = load_data(training_file_path)\n",
    "test_data = load_data(test_file_path)\n",
    "dev_data = load_data(dev_file_path)\n",
    "\n",
    "# Identifying unique labels for POS tags and negation scopes.\n",
    "# Extracting and combining POS tags from training and development datasets.\n",
    "training_pos_tags = training_data['pos']\n",
    "dev_pos_tags = dev_data['pos']\n",
    "combined_pos_tags = pd.concat([training_pos_tags, dev_pos_tags])\n",
    "unique_pos_tags = sorted(combined_pos_tags.unique().tolist())\n",
    "\n",
    "# Creating a mapping of POS tags and negation scopes to unique indices.\n",
    "pos_label_mapping = {label: idx for idx, label in enumerate(unique_pos_tags)}\n",
    "unique_negation_scopes = sorted(training_data['negation_scope'].unique().tolist())\n",
    "negation_scope_label_mapping = {label: idx for idx, label in enumerate(unique_negation_scopes)}\n",
    "\n",
    "# Transforming the data to the required format.\n",
    "transformed_training_data = transform_data(training_data, pos_label_mapping, negation_scope_label_mapping)\n",
    "transformed_test_data = transform_data(test_data, pos_label_mapping, negation_scope_label_mapping)\n",
    "transformed_dev_data = transform_data(dev_data, pos_label_mapping, negation_scope_label_mapping)\n",
    "\n",
    "# Defining the features for the Hugging Face dataset.\n",
    "features = Features({\n",
    "    'id': Value(dtype='string'),\n",
    "    'tokens': Sequence(feature=Value(dtype='string')),\n",
    "    'lemmas': Sequence(feature=Value(dtype='string')),\n",
    "    'pos': Sequence(feature=ClassLabel(names=unique_pos_tags)),\n",
    "    'constituency': Sequence(feature=Value(dtype='string')),\n",
    "    'negation_cue': Sequence(feature=Value(dtype='string')),\n",
    "    'negation_scope': Sequence(feature=ClassLabel(names=unique_negation_scopes)),\n",
    "    'negated_event': Sequence(feature=Value(dtype='string')),\n",
    "})\n",
    "\n",
    "# Converting the transformed data to Hugging Face datasets.\n",
    "hf_training_dataset = Dataset.from_pandas(pd.DataFrame(transformed_training_data), features=features)\n",
    "hf_test_dataset = Dataset.from_pandas(pd.DataFrame(transformed_test_data), features=features)\n",
    "hf_dev_dataset = Dataset.from_pandas(pd.DataFrame(transformed_dev_data), features=features)\n",
    "\n",
    "# Creating a DatasetDict to hold the training, validation, and test datasets.\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': hf_training_dataset,\n",
    "    'validation': hf_dev_dataset,\n",
    "    'test': hf_test_dataset\n",
    "})\n",
    "\n",
    "# Printing the dataset dictionary to check its structure and contents\n",
    "# The datasets object itself is DatasetDict, which contains one key for the training, validation and test set\n",
    "# We can see the training, validation and test sets all have a column for the tokens (the input texts split into words) and one column of labels\n",
    "\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eXNLu_-nIrJI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the AutoTokenizer class from the transformers library.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initializing the tokenizer with a pre-trained model.\n",
    "# 'model_checkpoint' specifies the pre-trained model to use.\n",
    "# This tokenizer will be used to convert text into tokens that the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the transformers library.\n",
    "import transformers\n",
    "\n",
    "# Asserting that the 'tokenizer' object is an instance of PreTrainedTokenizerFast.\n",
    "# This assertion ensures that the tokenizer has the desired properties and functionalities\n",
    "# of the PreTrainedTokenizerFast class, which is optimized for speed and efficiency.\n",
    "# If the assertion fails, it will raise an AssertionError.\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We're now ready to write the function that will preprocess our samples. We feed them to the `tokenizer` with the argument `truncation=True` (to truncate texts that are bigger than the maximum size allowed by the model) and `is_split_into_words=True` (as seen above). Then we align the labels with the token ids using the strategy we picked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vc0BSBLIIrJQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_max_length_across_datasets(dataset_dict):\n",
    "    # This function is added to the block in order to maintain a consistent max_length for padding.\n",
    "    max_length = 0\n",
    "    for dataset_name, dataset in dataset_dict.items():\n",
    "        for example in dataset:\n",
    "            # 'tokens' is assumed to be the key for tokenized text\n",
    "            length = len(example[\"tokens\"])\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "    return max_length\n",
    "# We find the longest input througout the dataset and keep that as the input length\n",
    "max_length = find_max_length_across_datasets(dataset_dict)\n",
    "    \n",
    "def tokenize_and_align_labels(examples):\n",
    "    \n",
    "    # Tokenizing the input text. 'truncation=True' ensures inputs fit model max size.\n",
    "    # 'is_split_into_words=True' indicates the input is already split into words.\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='max_length', max_length=max_length)\n",
    "\n",
    "    labels = []  # Initialize a list to store aligned labels for each tokenized input.\n",
    "\n",
    "    # Iterate over each example. 'enumerate' provides a counter 'i' and the 'label' (negation scope).\n",
    "    for i, label in enumerate(examples[\"negation_scope\"]):\n",
    "        # Get word IDs for each token to map them back to their respective words.\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None  # Initialize a variable to track the previous word index.\n",
    "        label_ids = []  # List to hold the labels for each token in the current example.\n",
    "\n",
    "        # Iterate over each word ID in 'word_ids'.\n",
    "        for word_idx in word_ids:\n",
    "            # Assign -100 to special tokens (word ID is None), which are ignored in loss calculation.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Assign the label to the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For subsequent tokens of the same word:\n",
    "            else:\n",
    "                # If 'label_all_tokens' is True, use the same label; otherwise, use -100.\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx  # Update the previous word index.\n",
    "\n",
    "        labels.append(label_ids)  # Add the list of labels for this example to the main list.\n",
    "\n",
    "    # Add the aligned labels to the tokenized inputs.\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs  # Return the tokenized inputs with aligned labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the 'tokenize_and_align_labels' function to the 'dataset_dict' with batch processing (batched=True).\n",
    "tokenized_datasets = dataset_dict.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about token classification, we use the `AutoModelForTokenClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which we can get from the features, as seen before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and classes from the 'transformers' library.\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Extracting the list of Negation Scope tag names from the 'hf_training_dataset'.\n",
    "# This is done by accessing the 'negation_scope' feature in the dataset's features, \n",
    "# and then retrieving the names associated with this feature.\n",
    "# The result is stored in 'label_list', which will contain all the unique Negation Scope tags \n",
    "# used in the training dataset.\n",
    "label_list = hf_training_dataset.features[\"negation_scope\"].feature.names\n",
    "\n",
    "# Create a model instance using 'AutoModelForTokenClassification' and load the pretrained model using 'from_pretrained'.\n",
    "# 'model_checkpoint' should contain the model name or path to the pretrained model.\n",
    "# 'num_labels' is the number of unique labels for your token classification task, obtained from the features.\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "# At this point, 'model' contains the pretrained model with the specified number of output labels for token classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Bliy8zgjIrJY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the model name from the 'model_checkpoint' path.\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "# Create a TrainingArguments object to configure the fine-tuning process.\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",  # Specify the output directory for the fine-tuned model.\n",
    "    evaluation_strategy=\"epoch\",        # Evaluation strategy, in this case, \"epoch\".\n",
    "    learning_rate=1e-4,                # Learning rate for fine-tuning.\n",
    "    per_device_train_batch_size=batch_size,  # Batch size for training on each device.\n",
    "    per_device_eval_batch_size=batch_size,   # Batch size for evaluation on each device.\n",
    "    num_train_epochs=1,                # Number of training epochs.\n",
    "    weight_decay=0.01,                 # Weight decay for regularization.\n",
    "    push_to_hub=False,                 # Whether or not to push the model to the Hugging Face Model Hub.\n",
    ")\n",
    "\n",
    "# After this code block, the 'args' object contains the configuration for the fine-tuning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will need a data collator that will batch our processed examples together while applying padding to make them all the same size (each pad will be padded to the length of its longest example). There is a data collator for this task in the Transformers library, that not only pads the inputs, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from evaluate import load\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. Here we will load the [`seqeval`](https://github.com/chakki-works/seqeval) metric (which is commonly used to evaluate results on the CONLL dataset) via the Datasets library.\n",
    "We changed the method of setting the metric as older method was going to be depracated : https://huggingface.co/docs/evaluate/choosing_a_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "So we will need to do a bit of post-processing on our predictions:\n",
    "- select the predicted index (with the maximum logit) for each token\n",
    "- convert it to its string label\n",
    "- ignore everywhere we set a label of -100\n",
    "\n",
    "The following function does all this post-processing on the result of `Trainer.evaluate` (which is a namedtuple containing predictions and labels) before applying the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function 'compute_metrics' that takes predictions and labels as input and calculates evaluation metrics.\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # Find the index with the maximum logit for each token (argmax over the last dimension).\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (-100), convert predicted and true labels to their string representations.\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Calculate evaluation metrics using 'metric.compute'.\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Return a dictionary containing precision, recall, F1 score, and accuracy.\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Note that we drop the precision/recall/f1 computed for each category and only focus on the overall precision/recall/f1/accuracy.\n",
    "\n",
    "Then we just need to pass all of this along with our datasets to the `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 10:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.060097</td>\n",
       "      <td>0.707547</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.979132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mumu/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: OS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/mumu/anaconda3/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: IS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=237, training_loss=0.08531076294460377, metrics={'train_runtime': 641.2396, 'train_samples_per_second': 5.895, 'train_steps_per_second': 0.37, 'total_flos': 80060749226640.0, 'train_loss': 0.08531076294460377, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKASz-2vIrJi"
   },
   "source": [
    "The `evaluate` method allows you to evaluate again on the evaluation dataset or on another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UOUcBkX8IrJi",
    "outputId": "de5b9dd6-9dc0-4702-cb43-55e9829fde25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06009732931852341,\n",
       " 'eval_precision': 0.7075471698113207,\n",
       " 'eval_recall': 0.7575757575757576,\n",
       " 'eval_f1': 0.7317073170731707,\n",
       " 'eval_accuracy': 0.9791321426379427,\n",
       " 'eval_runtime': 37.1309,\n",
       " 'eval_samples_per_second': 21.976,\n",
       " 'eval_steps_per_second': 1.374,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the precision/recall/f1 computed for each category now that we have finished training, we can apply the same function as before on the result of the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': {'precision': 0.7356687898089171,\n",
       "  'recall': 0.7777777777777778,\n",
       "  'f1': 0.7561374795417348,\n",
       "  'number': 297},\n",
       " 'overall_precision': 0.7356687898089171,\n",
       " 'overall_recall': 0.7777777777777778,\n",
       " 'overall_f1': 0.7561374795417348,\n",
       " 'overall_accuracy': 0.9797459031485914}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate word_id_lists\n",
    "word_id_lists = []\n",
    "for example in tokenized_datasets[\"validation\"]:\n",
    "    word_ids = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True).word_ids()\n",
    "    word_id_lists.append(word_ids)\n",
    "\n",
    "# Perform prediction\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Process predictions and labels\n",
    "updated_true_predictions = []\n",
    "updated_true_labels = []\n",
    "\n",
    "for prediction, label, word_ids in zip(predictions, labels, word_id_lists):\n",
    "    new_prediction = []\n",
    "    new_label = []\n",
    "    current_label = None\n",
    "    previous_word_id = None\n",
    "\n",
    "    for p, l, word_id in zip(prediction, label, word_ids):\n",
    "        if l != -100:  # Only consider non-special tokens\n",
    "            if word_id is None or word_id != previous_word_id:\n",
    "                current_label = label_list[p]\n",
    "            new_prediction.append(current_label)\n",
    "            new_label.append(label_list[l])\n",
    "            previous_word_id = word_id\n",
    "\n",
    "    updated_true_predictions.append(new_prediction)\n",
    "    updated_true_labels.append(new_label)\n",
    "\n",
    "# Compute results with the updated predictions and labels\n",
    "results = metric.compute(predictions=updated_true_predictions, references=updated_true_labels)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_list = []\n",
    "pred_list = []\n",
    "label_list = []\n",
    "sentence_id_list = []\n",
    "\n",
    "# This block saves all tokens, predictions and their gold labels with their sentence ids into lists \n",
    "# and writes them into a tsv file to use as input for our evaluation script.\n",
    "\n",
    "for i in range(len(hf_dev_dataset)):\n",
    "    sentence = hf_dev_dataset[i]['tokens']\n",
    "    sentence_id = hf_dev_dataset[i]['id']\n",
    "    tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "    no_cls_tokens = tokens[1:-1]\n",
    "    for token in no_cls_tokens:\n",
    "        token_list.append(token)\n",
    "        sentence_id_list.append(sentence_id)\n",
    "    token_list.append(' ')\n",
    "    sentence_id_list.append(' ')\n",
    "\n",
    "\n",
    "for pred, lab in zip(updated_true_predictions, updated_true_labels):\n",
    "    # Iterate through the tokens in each sentence pair.\n",
    "    for left_token, right_token in zip(pred, lab):\n",
    "        pred_list.append(left_token)\n",
    "        label_list.append(right_token)\n",
    "    pred_list.append('\\n')\n",
    "    label_list.append('\\n')\n",
    "    \n",
    "# Define the file name where you want to save the TSV file.\n",
    "output_file = \"../bert/data/predictions_260124_last.tsv\"\n",
    "\n",
    "# Open the file for writing in TSV format.\n",
    "with open(output_file, \"w\") as tsv_file:\n",
    "    for sent_id, token, lab, pred in zip(sentence_id_list,token_list,label_list,pred_list):\n",
    "        if sent_id != ' ' and token != ' ' and lab != ' ' and pred != ' ':\n",
    "\n",
    "            line = f\"{sent_id}\\t{token}\\t{lab}\\t{pred}\"\n",
    "            tsv_file.write(line + \"\\n\")       \n",
    "        else:\n",
    "            tsv_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
